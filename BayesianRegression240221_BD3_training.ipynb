{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi LInear Regression\n",
    "\n",
    "## Data: FinalDataset.csv\n",
    "## X: Urban spatial data of 500m around a subway station\n",
    "## Y: From Fast Fourier Transformation, magnitude of each frequecy of every 10 interver from 0 to 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.nn import PyroModule, PyroSample\n",
    "from pyro.infer import MCMC, NUTS\n",
    "import pyro.distributions.constraints as constraints\n",
    "from pyro.infer.autoguide import AutoMultivariateNormal, init_to_mean\n",
    "from pyro.infer import SVI, Trace_ELBO, Predictive\n",
    "import pyro.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elbo loss: 10426438.794331133\n",
      "elbo loss: 229159.79158104956\n",
      "elbo loss: 27337.9087305665\n",
      "elbo loss: 19283.86172556877\n",
      "elbo loss: 14545.925424039364\n",
      "elbo loss: 13861.835511505604\n",
      "elbo loss: 8830.813659667969\n",
      "elbo loss: 7283.879822969437\n",
      "elbo loss: 3459.7282358407974\n",
      "elbo loss: 2857.507586836815\n",
      "elbo loss: 2085.7125931978226\n",
      "elbo loss: 1846.1097767353058\n",
      "elbo loss: 1917.0328586101532\n",
      "elbo loss: 1654.5804405212402\n",
      "elbo loss: 1496.9228782653809\n",
      "elbo loss: 1617.3575872182846\n",
      "elbo loss: 1310.5873589515686\n",
      "elbo loss: 1270.7773673534393\n",
      "elbo loss: 1269.835916876793\n",
      "elbo loss: 1235.9197667837143\n",
      "elbo loss: 1218.3066148757935\n",
      "elbo loss: 1218.6463344097137\n",
      "elbo loss: 1228.0793342590332\n",
      "elbo loss: 1217.9160171747208\n",
      "elbo loss: 1215.3825497627258\n",
      "elbo loss: 1219.4228105545044\n",
      "elbo loss: 1212.5256447792053\n",
      "elbo loss: 1210.5537691116333\n",
      "elbo loss: 1207.1943620443344\n",
      "elbo loss: 1204.5195100307465\n",
      "elbo loss: 1201.15027654171\n",
      "elbo loss: 1199.4208145141602\n",
      "elbo loss: 1188.5569610595703\n",
      "elbo loss: 1181.902899980545\n",
      "elbo loss: 1175.1714684963226\n",
      "elbo loss: 1168.0961827039719\n",
      "elbo loss: 1161.8315770626068\n",
      "elbo loss: 1155.9668793678284\n",
      "elbo loss: 1150.7294672727585\n",
      "elbo loss: 1145.822107076645\n",
      "elbo loss: 1140.7851564884186\n",
      "elbo loss: 1136.3958817720413\n",
      "elbo loss: 1131.840947985649\n",
      "elbo loss: 1128.11614215374\n",
      "elbo loss: 1124.4739664793015\n",
      "elbo loss: 1121.2257076501846\n",
      "elbo loss: 1118.5746250152588\n",
      "elbo loss: 1116.9863164424896\n",
      "elbo loss: 1115.872843503952\n",
      "elbo loss: 1115.127250790596\n",
      "elbo loss: 1114.4459978342056\n",
      "elbo loss: 1114.0133957862854\n",
      "elbo loss: 1114.8091748952866\n",
      "elbo loss: 1113.9038296937943\n",
      "elbo loss: 1114.4366102218628\n",
      "elbo loss: 1114.576368689537\n",
      "elbo loss: 1113.9846467971802\n",
      "elbo loss: 1113.7329596281052\n",
      "elbo loss: 1114.8017052412033\n",
      "elbo loss: 1113.9472279548645\n",
      "elbo loss: 97449.07558113337\n",
      "elbo loss: 1849.337137028575\n",
      "elbo loss: 615.7587254047394\n",
      "elbo loss: 562.6319525241852\n",
      "elbo loss: 511.91773104667664\n",
      "elbo loss: 456.4652574658394\n",
      "elbo loss: 427.60715013742447\n",
      "elbo loss: 399.9568131864071\n",
      "elbo loss: 383.18054661154747\n",
      "elbo loss: 344.6201236099005\n",
      "elbo loss: 309.9018581956625\n",
      "elbo loss: 269.54901131242514\n",
      "elbo loss: 230.93988624215126\n",
      "elbo loss: 201.96662682294846\n",
      "elbo loss: 187.00062331557274\n",
      "elbo loss: 185.58285173773766\n",
      "elbo loss: 183.91279718279839\n",
      "elbo loss: 184.02425748109818\n",
      "elbo loss: 183.3710169494152\n",
      "elbo loss: 183.94256508350372\n",
      "elbo loss: 183.8857154250145\n",
      "elbo loss: 184.27158683538437\n",
      "elbo loss: 183.56339180469513\n",
      "elbo loss: 183.66213396191597\n",
      "elbo loss: 183.68412855267525\n",
      "elbo loss: 183.34812247753143\n",
      "elbo loss: 183.68314108252525\n",
      "elbo loss: 183.51800927519798\n",
      "elbo loss: 183.5481560230255\n",
      "elbo loss: 183.80384308099747\n",
      "elbo loss: 183.8389133810997\n",
      "elbo loss: 183.23798489570618\n",
      "elbo loss: 183.72578886151314\n",
      "elbo loss: 183.72097992897034\n",
      "elbo loss: 183.80967065691948\n",
      "elbo loss: 184.02327728271484\n",
      "elbo loss: 184.09593257308006\n",
      "elbo loss: 183.8613932132721\n",
      "elbo loss: 183.5697262585163\n",
      "elbo loss: 183.76384636759758\n",
      "elbo loss: 183.64576223492622\n",
      "elbo loss: 183.9293164908886\n",
      "elbo loss: 183.66266268491745\n",
      "elbo loss: 183.93584537506104\n",
      "elbo loss: 183.8638852238655\n",
      "elbo loss: 183.7136627137661\n",
      "elbo loss: 183.5448179244995\n",
      "elbo loss: 183.701546728611\n",
      "elbo loss: 183.79516905546188\n",
      "elbo loss: 183.64180698990822\n",
      "elbo loss: 183.3176385462284\n",
      "elbo loss: 183.43881586194038\n",
      "elbo loss: 184.48923206329346\n",
      "elbo loss: 183.64790552854538\n",
      "elbo loss: 184.2053283751011\n",
      "elbo loss: 184.3471780717373\n",
      "elbo loss: 183.74976921081543\n",
      "elbo loss: 183.50286149978638\n",
      "elbo loss: 184.57515439391136\n",
      "elbo loss: 183.71789905428886\n",
      "elbo loss: 9409205.794331133\n",
      "elbo loss: 200178.12603142858\n",
      "elbo loss: 23879.624860167503\n",
      "elbo loss: 16924.914015352726\n",
      "elbo loss: 12826.923302352428\n",
      "elbo loss: 12279.278712034225\n",
      "elbo loss: 7860.638499498367\n",
      "elbo loss: 6513.422140002251\n",
      "elbo loss: 3132.7026703357697\n",
      "elbo loss: 2606.612132668495\n",
      "elbo loss: 1928.5946760177612\n",
      "elbo loss: 1720.2841007709503\n",
      "elbo loss: 1785.9286587238312\n",
      "elbo loss: 1556.0173897743225\n",
      "elbo loss: 1418.7140362262726\n",
      "elbo loss: 1527.422710776329\n",
      "elbo loss: 1258.0708329677582\n",
      "elbo loss: 1224.585007071495\n",
      "elbo loss: 1224.268708229065\n",
      "elbo loss: 1196.0225715637207\n",
      "elbo loss: 1182.844128370285\n",
      "elbo loss: 1184.6500630378723\n",
      "elbo loss: 1189.4054510593414\n",
      "elbo loss: 1180.7616159915924\n",
      "elbo loss: 1178.3050374984741\n",
      "elbo loss: 1182.2791001796722\n",
      "elbo loss: 1173.9136946201324\n",
      "elbo loss: 1170.5415437221527\n",
      "elbo loss: 1165.8775835037231\n",
      "elbo loss: 1161.439673423767\n",
      "elbo loss: 1155.5250478982925\n",
      "elbo loss: 1149.665155172348\n",
      "elbo loss: 1132.6664243936539\n",
      "elbo loss: 1117.447420835495\n",
      "elbo loss: 1101.5849944353104\n",
      "elbo loss: 1089.4288402795792\n",
      "elbo loss: 1082.0107015371323\n",
      "elbo loss: 1077.67440700531\n",
      "elbo loss: 1075.613904953003\n",
      "elbo loss: 1074.1216422319412\n",
      "elbo loss: 1072.9205733537674\n",
      "elbo loss: 1072.4687881469727\n",
      "elbo loss: 1071.6028671264648\n",
      "elbo loss: 1071.4257657527924\n",
      "elbo loss: 1070.9885634183884\n",
      "elbo loss: 1070.9130800962448\n",
      "elbo loss: 1070.6799099445343\n",
      "elbo loss: 1070.7234480381012\n",
      "elbo loss: 1070.7183984518051\n",
      "elbo loss: 1070.6580737829208\n",
      "elbo loss: 1070.2749062776566\n",
      "elbo loss: 1070.3333587646484\n",
      "elbo loss: 1071.3615483045578\n",
      "elbo loss: 1070.508309841156\n",
      "elbo loss: 1071.0562142133713\n",
      "elbo loss: 1071.197890162468\n",
      "elbo loss: 1070.6048443317413\n",
      "elbo loss: 1070.353281378746\n",
      "elbo loss: 1071.4227302074432\n",
      "elbo loss: 1070.567927122116\n",
      "elbo loss: 96894.57558113337\n",
      "elbo loss: 1846.8029814362526\n",
      "elbo loss: 609.0237894654274\n",
      "elbo loss: 556.2978218197823\n",
      "elbo loss: 488.5818785727024\n",
      "elbo loss: 443.20261722803116\n",
      "elbo loss: 415.58972108364105\n",
      "elbo loss: 387.925821185112\n",
      "elbo loss: 369.9983342140913\n",
      "elbo loss: 328.2909500449896\n",
      "elbo loss: 290.8719973117113\n",
      "elbo loss: 250.49343316257\n",
      "elbo loss: 212.13011671602726\n",
      "elbo loss: 187.32580289244652\n",
      "elbo loss: 178.12938532233238\n",
      "elbo loss: 178.42733669281006\n",
      "elbo loss: 176.88675424456596\n",
      "elbo loss: 177.00581443309784\n",
      "elbo loss: 176.35381650924683\n",
      "elbo loss: 176.92469054460526\n",
      "elbo loss: 176.86397367715836\n",
      "elbo loss: 177.24912102520466\n",
      "elbo loss: 176.5450465977192\n",
      "elbo loss: 176.64436864852905\n",
      "elbo loss: 176.66590082645416\n",
      "elbo loss: 176.32682418823242\n",
      "elbo loss: 176.66481283307076\n",
      "elbo loss: 176.49865126609802\n",
      "elbo loss: 176.5302795469761\n",
      "elbo loss: 176.7857073545456\n",
      "elbo loss: 176.80953106284142\n",
      "elbo loss: 176.2196960747242\n",
      "elbo loss: 176.70110857486725\n",
      "elbo loss: 176.70273759961128\n",
      "elbo loss: 176.79176408052444\n",
      "elbo loss: 177.0045235157013\n",
      "elbo loss: 177.070612937212\n",
      "elbo loss: 176.84316340088844\n",
      "elbo loss: 176.5516241788864\n",
      "elbo loss: 176.74572801589966\n",
      "elbo loss: 176.62767094373703\n",
      "elbo loss: 176.91132527589798\n",
      "elbo loss: 176.64457553625107\n",
      "elbo loss: 176.91775077581406\n",
      "elbo loss: 176.84583204984665\n",
      "elbo loss: 176.6972358226776\n",
      "elbo loss: 176.52651473879814\n",
      "elbo loss: 176.68192100524902\n",
      "elbo loss: 176.79720395803452\n",
      "elbo loss: 176.6237227320671\n",
      "elbo loss: 176.29954317212105\n",
      "elbo loss: 176.420634329319\n",
      "elbo loss: 177.47113499045372\n",
      "elbo loss: 176.62976995110512\n",
      "elbo loss: 177.18603414297104\n",
      "elbo loss: 177.3290015757084\n",
      "elbo loss: 176.73167383670807\n",
      "elbo loss: 176.48477229475975\n",
      "elbo loss: 177.55699533224106\n",
      "elbo loss: 176.69979482889175\n",
      "elbo loss: 10792.300190508366\n",
      "elbo loss: 500.29143135249615\n",
      "elbo loss: 422.39917516708374\n",
      "elbo loss: 414.44965437054634\n",
      "elbo loss: 412.1757041364908\n",
      "elbo loss: 416.2533810734749\n",
      "elbo loss: 413.6399389654398\n",
      "elbo loss: 413.23454836010933\n",
      "elbo loss: 410.16934847831726\n",
      "elbo loss: 409.77677407860756\n",
      "elbo loss: 409.9671195745468\n",
      "elbo loss: 409.07082337141037\n",
      "elbo loss: 408.87961199879646\n",
      "elbo loss: 408.67570908367634\n",
      "elbo loss: 408.8508971333504\n",
      "elbo loss: 408.7371677607298\n",
      "elbo loss: 408.5529564321041\n",
      "elbo loss: 408.7916771173477\n",
      "elbo loss: 408.5411714464426\n",
      "elbo loss: 408.8797547221184\n",
      "elbo loss: 407.7827808856964\n",
      "elbo loss: 409.16093823313713\n",
      "elbo loss: 408.5203028470278\n",
      "elbo loss: 408.5599133372307\n",
      "elbo loss: 408.5401834100485\n",
      "elbo loss: 408.5885287821293\n",
      "elbo loss: 408.62892343103886\n",
      "elbo loss: 408.2652860879898\n",
      "elbo loss: 408.4235107898712\n",
      "elbo loss: 408.59697514772415\n",
      "elbo loss: 408.5273002386093\n",
      "elbo loss: 408.02785581350327\n",
      "elbo loss: 408.5476123839617\n",
      "elbo loss: 408.523069947958\n",
      "elbo loss: 408.6258115321398\n",
      "elbo loss: 408.8417784720659\n",
      "elbo loss: 408.90932554006577\n",
      "elbo loss: 408.68556858599186\n",
      "elbo loss: 408.39208386838436\n",
      "elbo loss: 408.58647003769875\n",
      "elbo loss: 408.4690934717655\n",
      "elbo loss: 408.7511953115463\n",
      "elbo loss: 408.4853090196848\n",
      "elbo loss: 408.75872734189034\n",
      "elbo loss: 408.69142942130566\n",
      "elbo loss: 408.53606717288494\n",
      "elbo loss: 408.3672627657652\n",
      "elbo loss: 408.5227873623371\n",
      "elbo loss: 408.61801977455616\n",
      "elbo loss: 408.46421943604946\n",
      "elbo loss: 408.13994467258453\n",
      "elbo loss: 408.2613256573677\n",
      "elbo loss: 409.3116056919098\n",
      "elbo loss: 408.4700511097908\n",
      "elbo loss: 409.0278539508581\n",
      "elbo loss: 409.17089788615704\n",
      "elbo loss: 408.57333040237427\n",
      "elbo loss: 408.3254485428333\n",
      "elbo loss: 409.42086586356163\n",
      "elbo loss: 408.54053524136543\n",
      "elbo loss: 74720.60683113337\n",
      "elbo loss: 1417.9057928919792\n",
      "elbo loss: 534.5743077993393\n",
      "elbo loss: 470.41929614543915\n",
      "elbo loss: 422.4842509627342\n",
      "elbo loss: 385.67187182605267\n",
      "elbo loss: 362.2691121995449\n",
      "elbo loss: 334.17026972025633\n",
      "elbo loss: 308.7174810320139\n",
      "elbo loss: 273.7450534515083\n",
      "elbo loss: 253.73762886226177\n",
      "elbo loss: 230.50263924896717\n",
      "elbo loss: 208.3096076399088\n",
      "elbo loss: 199.65305218100548\n",
      "elbo loss: 197.94374558329582\n",
      "elbo loss: 198.86561000347137\n",
      "elbo loss: 197.76900839805603\n",
      "elbo loss: 197.97314962744713\n",
      "elbo loss: 197.4416535794735\n",
      "elbo loss: 197.9324819892645\n",
      "elbo loss: 197.6373607069254\n",
      "elbo loss: 197.9446716159582\n",
      "elbo loss: 197.55273142457008\n",
      "elbo loss: 197.66966170072556\n",
      "elbo loss: 197.66733157634735\n",
      "elbo loss: 197.34173521399498\n",
      "elbo loss: 197.6737055182457\n",
      "elbo loss: 197.4447629749775\n",
      "elbo loss: 197.54877176880836\n",
      "elbo loss: 197.7887606471777\n",
      "elbo loss: 197.76789724826813\n",
      "elbo loss: 197.21345980465412\n",
      "elbo loss: 197.70938023924828\n",
      "elbo loss: 197.70073133707047\n",
      "elbo loss: 197.7950544655323\n",
      "elbo loss: 198.00920087099075\n",
      "elbo loss: 198.0758211761713\n",
      "elbo loss: 197.87656968832016\n",
      "elbo loss: 197.5590961277485\n",
      "elbo loss: 197.7516863644123\n",
      "elbo loss: 197.66837188601494\n",
      "elbo loss: 197.91713532805443\n",
      "elbo loss: 197.65063762664795\n",
      "elbo loss: 197.9238147288561\n",
      "elbo loss: 197.85194736719131\n",
      "elbo loss: 197.70155322551727\n",
      "elbo loss: 197.53256472945213\n",
      "elbo loss: 197.68809601664543\n",
      "elbo loss: 197.78317408263683\n",
      "elbo loss: 197.6299535781145\n",
      "elbo loss: 197.3055973649025\n",
      "elbo loss: 197.42667815089226\n",
      "elbo loss: 198.4772134423256\n",
      "elbo loss: 197.63666565716267\n",
      "elbo loss: 198.19321638345718\n",
      "elbo loss: 198.33518007397652\n",
      "elbo loss: 197.73783794045448\n",
      "elbo loss: 197.49078759551048\n",
      "elbo loss: 198.56408593058586\n",
      "elbo loss: 197.70589336752892\n",
      "elbo loss: 2762208.7943311334\n",
      "elbo loss: 58898.608936563134\n",
      "elbo loss: 7292.258073985577\n",
      "elbo loss: 5311.079757332802\n",
      "elbo loss: 4160.4874411821365\n",
      "elbo loss: 4025.2116550803185\n",
      "elbo loss: 2778.8284335136414\n",
      "elbo loss: 2410.2477318048477\n",
      "elbo loss: 1472.0469188690186\n",
      "elbo loss: 1338.2267099618912\n",
      "elbo loss: 1170.091938495636\n",
      "elbo loss: 1125.4773693084717\n",
      "elbo loss: 1146.4124009609222\n",
      "elbo loss: 1097.5176314115524\n",
      "elbo loss: 1072.3842188119888\n",
      "elbo loss: 1101.8049615621567\n",
      "elbo loss: 1051.287200331688\n",
      "elbo loss: 1049.3218793869019\n",
      "elbo loss: 1048.1147130727768\n",
      "elbo loss: 1046.789606809616\n",
      "elbo loss: 1051.303542137146\n",
      "elbo loss: 1054.1770700216293\n",
      "elbo loss: 1038.7590042352676\n",
      "elbo loss: 1034.4197645187378\n",
      "elbo loss: 1030.27574801445\n",
      "elbo loss: 1030.2669199705124\n",
      "elbo loss: 1020.2660691738129\n",
      "elbo loss: 1016.0181392431259\n",
      "elbo loss: 1012.0072945356369\n",
      "elbo loss: 1008.837543129921\n",
      "elbo loss: 1006.1484326124191\n",
      "elbo loss: 1004.5022631883621\n",
      "elbo loss: 1001.3328431844711\n",
      "elbo loss: 1000.038094162941\n",
      "elbo loss: 999.1882536411285\n",
      "elbo loss: 998.3666967153549\n",
      "elbo loss: 997.9618046283722\n",
      "elbo loss: 997.5556620359421\n",
      "elbo loss: 997.380952835083\n",
      "elbo loss: 997.3530356884003\n",
      "elbo loss: 997.2617868185043\n",
      "elbo loss: 997.658909201622\n",
      "elbo loss: 997.1781202554703\n",
      "elbo loss: 997.4654219150543\n",
      "elbo loss: 997.3151866197586\n",
      "elbo loss: 997.2540496587753\n",
      "elbo loss: 997.0553376674652\n",
      "elbo loss: 997.1955733299255\n",
      "elbo loss: 997.290379524231\n",
      "elbo loss: 997.148418545723\n",
      "elbo loss: 996.8186258077621\n",
      "elbo loss: 996.9395025968552\n",
      "elbo loss: 997.9875555038452\n",
      "elbo loss: 997.1451826095581\n",
      "elbo loss: 997.7053511142731\n",
      "elbo loss: 997.8551967144012\n",
      "elbo loss: 997.2572833299637\n",
      "elbo loss: 997.0036948919296\n",
      "elbo loss: 998.075892329216\n",
      "elbo loss: 997.2186008691788\n",
      "elbo loss: 74164.81776863337\n",
      "elbo loss: 1398.8453616797924\n",
      "elbo loss: 530.1907579600811\n",
      "elbo loss: 460.6658650636673\n",
      "elbo loss: 416.79610800743103\n",
      "elbo loss: 380.9340310692787\n",
      "elbo loss: 357.4043108224869\n",
      "elbo loss: 328.3325234204531\n",
      "elbo loss: 301.87351398169994\n",
      "elbo loss: 266.4443821385503\n",
      "elbo loss: 246.81336045265198\n",
      "elbo loss: 224.08341012895107\n",
      "elbo loss: 203.38715116679668\n",
      "elbo loss: 196.1397080719471\n",
      "elbo loss: 194.96823911368847\n",
      "elbo loss: 195.97233709692955\n",
      "elbo loss: 194.89082044363022\n",
      "elbo loss: 195.09733042120934\n",
      "elbo loss: 194.56809675693512\n",
      "elbo loss: 195.0570855140686\n",
      "elbo loss: 194.7569216787815\n",
      "elbo loss: 195.06233090162277\n",
      "elbo loss: 194.67739009857178\n",
      "elbo loss: 194.79441392421722\n",
      "elbo loss: 194.79161274433136\n",
      "elbo loss: 194.46470642089844\n",
      "elbo loss: 194.798216432333\n",
      "elbo loss: 194.56809702515602\n",
      "elbo loss: 194.67346492409706\n",
      "elbo loss: 194.91300283372402\n",
      "elbo loss: 194.8913122266531\n",
      "elbo loss: 194.33757378160954\n",
      "elbo loss: 194.83384329080582\n",
      "elbo loss: 194.82761961221695\n",
      "elbo loss: 194.91936987638474\n",
      "elbo loss: 195.13340014219284\n",
      "elbo loss: 195.20023718476295\n",
      "elbo loss: 194.97425067424774\n",
      "elbo loss: 194.6822773516178\n",
      "elbo loss: 194.87609019875526\n",
      "elbo loss: 194.75842249393463\n",
      "elbo loss: 195.04152378439903\n",
      "elbo loss: 194.77502137422562\n",
      "elbo loss: 195.04826159775257\n",
      "elbo loss: 194.97635692358017\n",
      "elbo loss: 194.82945880293846\n",
      "elbo loss: 194.65695306658745\n",
      "elbo loss: 194.8124323785305\n",
      "elbo loss: 194.90753787755966\n",
      "elbo loss: 194.7541697025299\n",
      "elbo loss: 194.4299595952034\n",
      "elbo loss: 194.55104330182076\n",
      "elbo loss: 195.60165777802467\n",
      "elbo loss: 194.78829422593117\n",
      "elbo loss: 195.32106763124466\n",
      "elbo loss: 195.45959731936455\n",
      "elbo loss: 194.86948055028915\n",
      "elbo loss: 194.61523568630219\n",
      "elbo loss: 195.68751719594002\n",
      "elbo loss: 194.83026933670044\n"
     ]
    }
   ],
   "source": [
    "date_types = ['wd_sc', 'wd_hc', 'we_sc', 'we_hc']\n",
    "targets = ['min_ride', 'max_ride']\n",
    "clusters = [['석촌', '태릉입구', '홍대입구', '강남', '선릉', '역삼', '교대', '공덕', '수유', '상봉', '중화', '상도']]\n",
    "\n",
    "def prepare_counts_df(predictive):\n",
    "    counts = predictive['coefs']\n",
    "    counts_mean = counts.mean()\n",
    "    counts_std = counts.std()\n",
    "\n",
    "    mean_dict = {'mean': counts_mean}\n",
    "    std_dict = {'std': counts_std}\n",
    "\n",
    "    mean_df = pd.DataFrame(mean_dict, index = [0])\n",
    "    std_df = pd.DataFrame(std_dict, index = [0])\n",
    "\n",
    "    return mean_df, std_df\n",
    "\n",
    "for date_type in date_types:\n",
    "    df = pd.read_csv(f'../Data/240103_data/train_{date_type}.csv', index_col = 0)\n",
    "    for target in targets:\n",
    "        x = df.iloc[:, 2:-8]\n",
    "        x = x.fillna(0)\n",
    "        if target == 'min_ride':\n",
    "            y = df[target]\n",
    "        elif target == 'max_ride':\n",
    "            y = np.log(df[target])\n",
    "            target = 'log_max_ride'\n",
    "        for cluster_number in range(len(clusters)):\n",
    "            test_stations = clusters[cluster_number]\n",
    "            test_id = df[df['name'].isin(test_stations)].index\n",
    "            train_id = df[~df['name'].isin(test_stations)].index\n",
    "            x_test = x.loc[test_id, :]\n",
    "            x_train = x.loc[train_id, :]\n",
    "            y_test = y.loc[test_id]\n",
    "            y_train = y.loc[train_id]\n",
    "            y_test_np = y_test\n",
    "            y_train_np = y_train\n",
    "\n",
    "            x_train = torch.tensor(x_train.values, dtype = torch.float)\n",
    "            x_test = torch.tensor(x_test.values, dtype = torch.float)\n",
    "            y_train = torch.tensor(y_train_np.values, dtype = torch.float)\n",
    "            y_test = torch.tensor(y_test_np.values, dtype = torch.float)\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            array_y_test = y_test.numpy()\n",
    "            y_test = pd.DataFrame(array_y_test)\n",
    "            y_test.to_csv(f'../Result/240116_LastCluster/Cluster{cluster_number+1}/test_{target}_{date_type}.csv')\n",
    "\n",
    "            class BNN(nn.Module):\n",
    "                def __init__(self, input_dim = 17, output_dim = 1, prior_scale = 10.):\n",
    "                    super(BNN, self).__init__()\n",
    "                    \n",
    "                    self.activation = nn.ReLU()\n",
    "                    self.layer1 = PyroModule[nn.Linear](input_dim, output_dim)\n",
    "                    self.input_dim = input_dim\n",
    "                    self.output_dim = output_dim\n",
    "\n",
    "                    # Set Layer parameters as random variables\n",
    "                    self.layer1.weight1 = PyroSample(dist.Normal(0., prior_scale).expand([output_dim, input_dim]).to_event(2))\n",
    "                    self.layer1.bias1 = PyroSample(dist.Normal(0., prior_scale).expand([output_dim]).to_event(1))\n",
    "                    \n",
    "                def forward(self, x, y = None):\n",
    "                    mu = self.activation(self.layer1(x)).squeeze(-1)\n",
    "                    sigma = pyro.sample(\"sigma\", dist.Gamma(.5, 1))\n",
    "                    with pyro.plate(\"data\", size = mu.shape[0]):\n",
    "                        pyro.sample(\"coefs\", dist.Normal(mu, sigma * sigma), obs = y)\n",
    "                    #return mu\n",
    "\n",
    "            model = BNN()\n",
    "            pyro.set_rng_seed(42)\n",
    "\n",
    "            \n",
    "            ###svi\n",
    "            guide = AutoMultivariateNormal(model, init_loc_fn = init_to_mean)\n",
    "            svi = SVI(model, guide, optim.Adam({\"lr\": .01}), loss = Trace_ELBO(vectorize_particles = True))\n",
    "\n",
    "            # Convert data to PyTorch tensors\n",
    "            x_train = np.array(x_train)\n",
    "            y_train = np.array(y_train)\n",
    "            x_train = torch.from_numpy(x_train).float()\n",
    "            y_train = torch.from_numpy(y_train).float()\n",
    "\n",
    "            pyro.clear_param_store()\n",
    "            num_iters = 30000\n",
    "            for i in range(num_iters):\n",
    "                elbo = svi.step(x_train, y_train)\n",
    "                if i % 500 == 0:\n",
    "                    print(\"elbo loss: {}\".format(elbo))\n",
    "\n",
    "            x_test = np.array(x_test)\n",
    "\n",
    "            x_test = torch.from_numpy(x_test).float()\n",
    "            y_test = np.array(y_test)\n",
    "            y_test = torch.from_numpy(y_test).float()\n",
    "\n",
    "            mean_dfs = []\n",
    "            std_dfs = []\n",
    "            for i in range(len(x_test)):\n",
    "                pred_svi = Predictive(model, guide = guide, num_samples = 5000)(x_test[i].reshape(1, 17))\n",
    "                mean_df_, std_df_ = prepare_counts_df(pred_svi)\n",
    "                mean_dfs.append(mean_df_)\n",
    "                std_dfs.append(std_df_)\n",
    "\n",
    "            mean_concat = pd.concat(mean_dfs, axis = 0)\n",
    "            std_concat = pd.concat(std_dfs, axis = 0)\n",
    "\n",
    "            mean_concat.to_csv(f'../Result/240116_LastCluster/Cluster{cluster_number+1}/pred_{target}_{date_type}_mean.csv')\n",
    "            std_concat.to_csv(f'../Result/240116_LastCluster/Cluster{cluster_number+1}/pred_{target}_{date_type}_std.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 12]' is invalid for input of size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4002823/773966729.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mguide\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m17\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[1, 12]' is invalid for input of size 1"
     ]
    }
   ],
   "source": [
    "guide(x_test[0].reshape(1, 17), y_test[0].reshape(1, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma: (5000, 1)\n",
      "coefs: (5000, 1)\n"
     ]
    }
   ],
   "source": [
    "pred_svi = Predictive(model, guide = guide, num_samples = 5000)(x_test[0].reshape(1, 17))\n",
    "for k, v in pred_svi.items():\n",
    "    print(f\"{k}: {tuple(v.shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_counts_df(predictive):\n",
    "    counts = predictive['coefs']\n",
    "    counts_mean = counts.mean()\n",
    "    counts_std = counts.std()\n",
    "\n",
    "    mean_dict = {'mean': counts_mean}\n",
    "    std_dict = {'std': counts_std}\n",
    "\n",
    "    mean_df = pd.DataFrame(mean_dict, index = [0])\n",
    "    std_df = pd.DataFrame(std_dict, index = [0])\n",
    "\n",
    "    return mean_df, std_df\n",
    "\n",
    "mean_dfs = []\n",
    "std_dfs = []\n",
    "for i in range(len(x_test)):\n",
    "    pred_svi = Predictive(model, guide = guide, num_samples = 5000)(x_test[i].reshape(1, 17))\n",
    "    mean_df_, std_df_ = prepare_counts_df(pred_svi)\n",
    "    mean_dfs.append(mean_df_)\n",
    "    std_dfs.append(std_df_)\n",
    "\n",
    "mean_concat = pd.concat(mean_dfs, axis = 0)\n",
    "std_concat = pd.concat(std_dfs, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_concat.to_csv(f'../Result/240103_Multi+Bayesian Quantity/pred_{target}_{date_type}_mean.csv')\n",
    "std_concat.to_csv(f'../Result/240103_Multi+Bayesian Quantity/pred_{target}_{date_type}_std.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BNN(nn.Module):\n",
    "    def __init__(self, input_dim = 15, output_dim = 1, prior_scale = 10.):\n",
    "        super(BNN, self).__init__()\n",
    "        \n",
    "        self.activation = nn.ReLU()\n",
    "        self.layer1 = PyroModule[nn.Linear](input_dim, output_dim)\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # Set Layer parameters as random variables\n",
    "        self.layer1.weight1 = PyroSample(dist.Normal(0., prior_scale).expand([output_dim, input_dim]).to_event(2))\n",
    "        self.layer1.bias1 = PyroSample(dist.Normal(0., prior_scale).expand([output_dim]).to_event(1))\n",
    "        \n",
    "    def forward(self, x, y = None):\n",
    "        mu = self.activation(self.layer1(x)).squeeze(-1)\n",
    "        sigma = pyro.sample(\"sigma\", dist.Gamma(.5, 1))\n",
    "        with pyro.plate(\"data\", size = mu.shape[0]):\n",
    "            coef = pyro.sample(\"coefs\", dist.Normal(mu, sigma * sigma), obs = y)\n",
    "        return coef\n",
    "\n",
    "model = BNN()\n",
    "pyro.set_rng_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 2000/2000 [00:05, 337.32it/s, step size=1.00e+00, acc. prob=0.935]\n"
     ]
    }
   ],
   "source": [
    "nuts_kernel = NUTS(model, jit_compile=False)\n",
    "mcmc = MCMC(nuts_kernel, num_samples=1000)\n",
    "mcmc.run(x_train, y_train)\n",
    "\n",
    "hmc_samples = {k: v.detach().cpu().numpy() for k, v in mcmc.get_samples().items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                mean       std    median      5.0%     95.0%     n_eff     r_hat\n",
      "     sigma      2.80      0.07      2.80      2.69      2.90    409.45      1.00\n",
      "\n",
      "Number of divergences: 0\n"
     ]
    }
   ],
   "source": [
    "mcmc.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['sigma'])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmc_samples.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_concat = pd.concat(mean_dfs, axis = 0)\n",
    "std_concat = pd.concat(std_dfs, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyro.poutine.trace_struct.Trace at 0x7faa34f531c0>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyro.poutine.trace(guide).get_trace(x_test[0].reshape(1, 15), y_test[0].reshape(1, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BNN(nn.Module):\n",
    "    def __init__(self, input_dim = 15, hidden_dim = 4, output_dim = 12, prior_scale = 10.):\n",
    "        super(BNN, self).__init__()\n",
    "        \n",
    "        self.activation = nn.ReLU()\n",
    "        self.layer1 = PyroModule[nn.Linear](input_dim, hidden_dim)\n",
    "        self.layer2 = PyroModule[nn.Linear](hidden_dim, output_dim)\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Set Layer parameters as random variables\n",
    "        self.layer1.weight1 = PyroSample(dist.Normal(0., prior_scale).expand([hidden_dim, input_dim]).to_event(2))\n",
    "        self.layer1.bias1 = PyroSample(dist.Normal(0., prior_scale).expand([hidden_dim]).to_event(1))\n",
    "        self.layer2.weight2 = PyroSample(dist.Normal(0., prior_scale).expand([output_dim, hidden_dim]).to_event(2))\n",
    "        self.layer2.bias2 = PyroSample(dist.Normal(0., prior_scale).expand([output_dim]).to_event(1))\n",
    "\n",
    "    def forward(self, x, y = None):\n",
    "        x = self.activation(self.layer1(x))\n",
    "        mu = self.layer2(x).squeeze(-1)\n",
    "        sigma = pyro.sample(\"sigma\", dist.Gamma(.5, 1))\n",
    "        with pyro.plate(\"data\", mu.shape[0]):\n",
    "            obs = pyro.sample(\"obs\", dist.Normal(mu, sigma * sigma).to_event(1), obs = y)\n",
    "        return mu\n",
    "    '''\n",
    "        with pyro.irange(\"data\", mu.shape[0]):\n",
    "            return pyro.sample(\"obs\", dist.MultivariateNormal(loc, scale_tril), obs = y)'''\n",
    "        #return mu\n",
    "\n",
    "model = BNN()\n",
    "pyro.set_rng_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warmup:   0%|          | 0/2000 [00:00, ?it/s]"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "HMC/NUTS does not support model with subsample sites.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3457206/1294104021.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnuts_kernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNUTS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjit_compile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmcmc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMCMC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnuts_kernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmcmc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/PCW_venv/lib/python3.9/site-packages/pyro/poutine/messenger.py\u001b[0m in \u001b[0;36m_context_wrap\u001b[0;34m(context, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_context_wrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/PCW_venv/lib/python3.9/site-packages/pyro/infer/mcmc/api.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m             \u001b[0;31m# requires_grad\", which happens with `jit_compile` under PyTorch 1.7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchain_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchain_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m                     \u001b[0mnum_samples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchain_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/PCW_venv/lib/python3.9/site-packages/pyro/infer/mcmc/api.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0mhook_w_logging\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_add_logging_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             for sample in _gen_samples(\n\u001b[0m\u001b[1;32m    224\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarmup_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/PCW_venv/lib/python3.9/site-packages/pyro/infer/mcmc/api.py\u001b[0m in \u001b[0;36m_gen_samples\u001b[0;34m(kernel, warmup_steps, num_samples, hook, chain_id, *args, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_gen_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarmup_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchain_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m     \u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwarmup_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m     \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0msave_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"save_params\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/PCW_venv/lib/python3.9/site-packages/pyro/infer/mcmc/hmc.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, warmup_steps, *args, **kwargs)\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warmup_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwarmup_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_model_properties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_params\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/PCW_venv/lib/python3.9/site-packages/pyro/infer/mcmc/hmc.py\u001b[0m in \u001b[0;36m_initialize_model_properties\u001b[0;34m(self, model_args, model_kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_initialize_model_properties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         init_params, potential_fn, transforms, trace = initialize_model(\n\u001b[0m\u001b[1;32m    280\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/PCW_venv/lib/python3.9/site-packages/pyro/infer/mcmc/util.py\u001b[0m in \u001b[0;36minitialize_model\u001b[0;34m(model, model_args, model_kwargs, transforms, max_plate_nesting, jit_compile, jit_options, skip_jit_warnings, num_chains, init_strategy, initial_params)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Subsample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubsample_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubsample_size\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m                 raise NotImplementedError(\n\u001b[0m\u001b[1;32m    442\u001b[0m                     \u001b[0;34m\"HMC/NUTS does not support model with subsample sites.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m                 )\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: HMC/NUTS does not support model with subsample sites."
     ]
    }
   ],
   "source": [
    "nuts_kernel = NUTS(model, jit_compile=False)\n",
    "mcmc = MCMC(nuts_kernel, num_samples=1000)\n",
    "mcmc.run(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictive = Predictive(model=model, posterior_samples=mcmc.get_samples())\n",
    "preds = predictive(x_test)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred['obs'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictive = Predictive(model, guide = guide, num_samples = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.array(x_test)\n",
    "x_test = torch.from_numpy(x_test).float()\n",
    "y_test = np.array(y_test)\n",
    "y_test = torch.from_numpy(y_test).float()\n",
    "\n",
    "pred = predictive(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer import Predictive\n",
    "\n",
    "\n",
    "num_samples = 1000\n",
    "predictive = Predictive(model, guide=guide, num_samples=num_samples)\n",
    "svi_samples = {k: v.reshape(num_samples).detach().cpu().numpy()\n",
    "               for k, v in predictive(x_train, y_train).items()\n",
    "               if k != \"obs\"\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svi_samples.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for site, values in svi_samples.items():\n",
    "    print(\"Site: {}\".format(site))\n",
    "    print(values, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictive(x_train, y_train).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultivariateNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MultivariateNN, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            # Sample hidden layer weights\n",
    "            hidden_weights = pyro.sample(\"hidden_weights\", dist.Normal(torch.zeros(self.input_dim, self.hidden_dim), torch.ones(self.input_dim, self.hidden_dim)))\n",
    "            # Apply activation function to first layer\n",
    "            hidden = torch.relu(self.linear1(x) @ hidden_weights)\n",
    "\n",
    "            # Sample output layer weights\n",
    "            output_weights = pyro.sample(\"output_weights\", dist.Normal(torch.zeros(self.hidden_dim, self.output_dim), torch.ones(self.hidden_dim, self.output_dim)))\n",
    "            # Calculate outputs\n",
    "            outputs = self.linear2(hidden) @ output_weights\n",
    "\n",
    "            # Define likelihood function\n",
    "            pyro.sample(\"obs\", dist.MultivariateNormal(outputs, torch.eye(self.output_dim)))\n",
    "\n",
    "# Define data and model\n",
    "x = torch.randn(10, 3)\n",
    "y = torch.randn(10, 2)\n",
    "model = MultivariateNN(3, 5, 2)\n",
    "\n",
    "# Train the model\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "for epoch in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    loss = pyro.infer.TraceMeanField_ELBO().loss(model, x, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Predict on new data\n",
    "new_x = torch.randn(1, 3)\n",
    "with torch.no_grad():\n",
    "    predictions = model(new_x)\n",
    "    print(f\"Predictions for new data: {predictions}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BNN()\n",
    "pyro.set_rng_seed(42)\n",
    "\n",
    "nuts_kernel = NUTS(model, jit_compile = True)\n",
    "\n",
    "mcmc = MCMC(nuts_kernel, num_samples = 100)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "x_train = torch.from_numpy(x_train).float()\n",
    "y_train = torch.from_numpy(y_train).float()\n",
    "\n",
    "mcmc.run(x_train)\n",
    "#posterior_samples = mcmc.get_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BNN(nn.Module):\n",
    "    def __init__(self, input_dim = 15, hidden_dim = 4, output_dim = 12, prior_scale = 10.):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.activation = nn.ReLU()\n",
    "        self.layer1 = PyroModule[nn.Linear](input_dim, hidden_dim)\n",
    "        self.layer2 = PyroModule[nn.Linear](hidden_dim, output_dim)\n",
    "\n",
    "        # Set Layer parameters as random variables\n",
    "        self.layer1.weight1 = PyroSample(dist.Normal(0., prior_scale).expand([hidden_dim, input_dim]).to_event(2))\n",
    "        self.layer1.bias1 = PyroSample(dist.Normal(0., prior_scale).expand([hidden_dim]).to_event(1))\n",
    "        self.layer2.weight2 = PyroSample(dist.Normal(0., prior_scale).expand([output_dim, hidden_dim]).to_event(2))\n",
    "        self.layer2.bias2 = PyroSample(dist.Normal(0., prior_scale).expand([output_dim]).to_event(1))\n",
    "    \n",
    "    def forward(self, x, y = None):\n",
    "        x = self.activation(self.layer1(x))\n",
    "        mu = self.layer2(x).squeeze(-1)\n",
    "        sigma = pyro.sample(\"sigma\", dist.Gamma(.5, 1))\n",
    "        with pyro.plate(\"data\", mu.shape[0]):\n",
    "            pyro.sample(\"obs\", dist.Normal(mu.shape[-1], sigma), obs = y)\n",
    "        return mu\n",
    "\n",
    "model = BNN()\n",
    "guide = AutoDiagonalNormal(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "for j in range(num_iterations):\n",
    "    # calculate the loss and take a gradient step\n",
    "    loss = svi.step(x_train, y_train)\n",
    "    if j % 100 == 0:\n",
    "        print(\"[iteration %04d] loss: %.4f\" % (j + 1, loss / len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate data\n",
    "x_obs = np.hstack([np.linspace(-0.2, 0.2, 500), np.linspace(0.6, 1, 500)])\n",
    "noise = 0.02 * np.random.randn(x_obs.shape[0])\n",
    "y_obs = x_obs + 0.3 * np.sin(2 * np.pi * (x_obs + noise)) + 0.3 * np.sin(4 * np.pi * (x_obs + noise)) + noise\n",
    "\n",
    "x_true = np.linspace(-0.5, 1.5, 1000)\n",
    "y_true = x_true + 0.3 * np.sin(2 * np.pi * x_true) + 0.3 * np.sin(4 * np.pi * x_true)\n",
    "\n",
    "# Set plot limits and labels\n",
    "xlims = [-0.5, 1.5]\n",
    "ylims = [-1.5, 2.5]\n",
    "\n",
    "# Create plot\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(x_true, y_true, 'b-', linewidth=3, label=\"True function\")\n",
    "ax.plot(x_obs, y_obs, 'ko', markersize=4, label=\"Observations\")\n",
    "ax.set_xlim(xlims)\n",
    "ax.set_ylim(ylims)\n",
    "ax.set_xlabel(\"X\", fontsize=30)\n",
    "ax.set_ylabel(\"Y\", fontsize=30)\n",
    "ax.legend(loc=4, fontsize=15, frameon=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.nn import PyroModule, PyroSample\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MyFirstBNN(PyroModule):\n",
    "    def __init__(self, in_dim=1, out_dim=1, hid_dim=5, prior_scale=10.):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = nn.Tanh()  # or nn.ReLU()\n",
    "        self.layer1 = PyroModule[nn.Linear](in_dim, hid_dim)  # Input to hidden layer\n",
    "        self.layer2 = PyroModule[nn.Linear](hid_dim, out_dim)  # Hidden to output layer\n",
    "\n",
    "        # Set layer parameters as random variables\n",
    "        self.layer1.weight = PyroSample(dist.Normal(0., prior_scale).expand([hid_dim, in_dim]).to_event(2))\n",
    "        self.layer1.bias = PyroSample(dist.Normal(0., prior_scale).expand([hid_dim]).to_event(1))\n",
    "        self.layer2.weight = PyroSample(dist.Normal(0., prior_scale).expand([out_dim, hid_dim]).to_event(2))\n",
    "        self.layer2.bias = PyroSample(dist.Normal(0., prior_scale).expand([out_dim]).to_event(1))\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        x = x.reshape(-1, 1)\n",
    "        x = self.activation(self.layer1(x))\n",
    "        mu = self.layer2(x).squeeze()\n",
    "        sigma = pyro.sample(\"sigma\", dist.Gamma(.5, 1))  # Infer the response noise\n",
    "\n",
    "        # Sampling model\n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            obs = pyro.sample(\"obs\", dist.Normal(mu, sigma * sigma), obs=y)\n",
    "        return mu\n",
    "    \n",
    "from pyro.infer import MCMC, NUTS\n",
    "\n",
    "model = MyFirstBNN()\n",
    "\n",
    "# Set Pyro random seed\n",
    "pyro.set_rng_seed(42)\n",
    "\n",
    "# Define Hamiltonian Monte Carlo (HMC) kernel\n",
    "# NUTS = \"No-U-Turn Sampler\" (https://arxiv.org/abs/1111.4246), gives HMC an adaptive step size\n",
    "nuts_kernel = NUTS(model, jit_compile=False)  # jit_compile=True is faster but requires PyTorch 1.6+\n",
    "\n",
    "# Define MCMC sampler, get 50 posterior samples\n",
    "mcmc = MCMC(nuts_kernel, num_samples=50)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "x_train = torch.from_numpy(x_obs).float()\n",
    "y_train = torch.from_numpy(y_obs).float()\n",
    "\n",
    "# Run MCMC\n",
    "mcmc.run(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer import Predictive\n",
    "\n",
    "predictive = Predictive(model=model, posterior_samples=mcmc.get_samples())\n",
    "x_test = torch.linspace(xlims[0], xlims[1], 3000)\n",
    "preds = predictive(x_test)\n",
    "\n",
    "def plot_predictions(preds):\n",
    "    y_pred = preds['obs'].T.detach().numpy().mean(axis=1)\n",
    "    y_std = preds['obs'].T.detach().numpy().std(axis=1)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    xlims = [-0.5, 1.5]\n",
    "    ylims = [-1.5, 2.5]\n",
    "    plt.xlim(xlims)\n",
    "    plt.ylim(ylims)\n",
    "    plt.xlabel(\"X\", fontsize=30)\n",
    "    plt.ylabel(\"Y\", fontsize=30)\n",
    "\n",
    "    ax.plot(x_true, y_true, 'b-', linewidth=3, label=\"true function\")\n",
    "    ax.plot(x_obs, y_obs, 'ko', markersize=4, label=\"observations\")\n",
    "    ax.plot(x_obs, y_obs, 'ko', markersize=3)\n",
    "    ax.plot(x_test, y_pred, '-', linewidth=3, color=\"#408765\", label=\"predictive mean\")\n",
    "    ax.fill_between(x_test, y_pred - 2 * y_std, y_pred + 2 * y_std, alpha=0.6, color='#86cfac', zorder=5)\n",
    "\n",
    "    plt.legend(loc=4, fontsize=15, frameon=False)\n",
    "\n",
    "plot_predictions(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.distributions.constraints as constraints\n",
    "\n",
    "smoke_test = ('CI' in os.environ)\n",
    "assert pyro.__version__.startswith('1.8.6')\n",
    "\n",
    "def model(data):\n",
    "    m = pyro.sample(\"m\", dist.Normal(0, 1))\n",
    "    sd = pyro.sample(\"sd\", dist.LogNormal(m, 1))\n",
    "    with pyro.plate(\"N\", len(data)):\n",
    "        pyro.sample(\"obs\", dist.Normal(m, sd), obs=data)\n",
    "\n",
    "data = torch.ones(10)\n",
    "pyro.render_model(model, model_args=(data,))\n",
    "\n",
    "graph = pyro.render_model(model, model_args=(data,), filename=\"model.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "\n",
    "# for CI testing\n",
    "smoke_test = ('CI' in os.environ)\n",
    "assert pyro.__version__.startswith('1.8.6')\n",
    "pyro.set_rng_seed(1)\n",
    "\n",
    "\n",
    "# Set matplotlib settings\n",
    "%matplotlib inline\n",
    "plt.style.use('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_URL = \"https://d2hg8soec8ck9v.cloudfront.net/datasets/rugged_data.csv\"\n",
    "data = pd.read_csv(DATA_URL, encoding=\"ISO-8859-1\")\n",
    "df = data[[\"cont_africa\", \"rugged\", \"rgdppc_2000\"]]\n",
    "df = df[np.isfinite(df.rgdppc_2000)]\n",
    "df[\"rgdppc_2000\"] = np.log(df[\"rgdppc_2000\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from pyro.nn import PyroModule\n",
    "\n",
    "assert issubclass(PyroModule[nn.Linear], nn.Linear)\n",
    "assert issubclass(PyroModule[nn.Linear], PyroModule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: Add a feature to capture the interaction between \"cont_africa\" and \"rugged\"\n",
    "df[\"cont_africa_x_rugged\"] = df[\"cont_africa\"] * df[\"rugged\"]\n",
    "data = torch.tensor(df[[\"cont_africa\", \"rugged\", \"cont_africa_x_rugged\", \"rgdppc_2000\"]].values,\n",
    "                        dtype=torch.float)\n",
    "x_data, y_data = data[:, :-1], data[:, -1]\n",
    "\n",
    "linear_reg_model = PyroModule[nn.Linear](3, 1)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction = 'sum')\n",
    "optim = torch.optim.Adam(linear_reg_model.parameters(), lr = 0.05)\n",
    "num_iterations = 1500 if not smoke_test else 2\n",
    "\n",
    "def train():\n",
    "    # run the model forward on the data\n",
    "    y_pred = linear_reg_model(x_data).squeeze(-1)\n",
    "    # calculate the mse loss\n",
    "    loss = loss_fn(y_pred, y_data)\n",
    "    # initialize gradients to zero\n",
    "    optim.zero_grad()\n",
    "    # backpropagate\n",
    "    loss.backward()\n",
    "    # take a gradient step\n",
    "    optim.step()\n",
    "    return loss\n",
    "\n",
    "for j in range(num_iterations):\n",
    "    loss = train()\n",
    "    if (j + 1) % 50 == 0:\n",
    "        print(\"[iteration %04d] loss: %.4f\" % (j + 1, loss.item()))\n",
    "\n",
    "\n",
    "# Inspect learned parameters\n",
    "print(\"Learned parameters:\")\n",
    "for name, param in linear_reg_model.named_parameters():\n",
    "    print(name, param.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.nn import PyroSample\n",
    "from pyro.infer.autoguide import AutoDiagonalNormal\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "\n",
    "class BayesianRegression(PyroModule):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.linear = PyroModule[nn.Linear](in_features, out_features)\n",
    "        self.linear.weight = PyroSample(dist.Normal(0., 1.).expand([out_features, in_features]).to_event(2))\n",
    "        self.linear.bias = PyroSample(dist.Normal(0., 10.).expand([out_features]).to_event(1))\n",
    "\n",
    "    def forward(self, x, y = None):\n",
    "        sigma = pyro.sample(\"sigma\", dist.Uniform(0., 10.))\n",
    "        mean = self.linear(x).squeeze(-1)\n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            obs = pyro.sample(\"obs\", dist.Normal(mean, sigma), obs = y)\n",
    "        return mean\n",
    "\n",
    "model = BayesianRegression(3, 1)\n",
    "guide = AutoDiagonalNormal(model)\n",
    "\n",
    "adam = pyro.optim.Adam({\"lr\": 0.03})\n",
    "svi = SVI(model, guide, adam, loss = Trace_ELBO())\n",
    "\n",
    "pyro.clear_param_store()\n",
    "for j in range(num_iterations):\n",
    "    # calculate the loss and take a gradient step\n",
    "    loss = svi.step(x_data, y_data)\n",
    "    if j % 100 == 0:\n",
    "        print(\"[iteration %04d] loss: %.4f\" % (j + 1, loss / len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guide.requires_grad_(False)\n",
    "\n",
    "for name, value in pyro.get_param_store().items():\n",
    "    print(name, pyro.param(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guide.quantiles([0.25, 0.5, 0.75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer import Predictive\n",
    "\n",
    "\n",
    "def summary(samples):\n",
    "    site_stats = {}\n",
    "    for k, v in samples.items():\n",
    "        site_stats[k] = {\n",
    "            \"mean\": torch.mean(v, 0),\n",
    "            \"std\": torch.std(v, 0),\n",
    "            \"5%\": v.kthvalue(int(len(v) * 0.05), dim=0)[0],\n",
    "            \"95%\": v.kthvalue(int(len(v) * 0.95), dim=0)[0],\n",
    "        }\n",
    "    return site_stats\n",
    "\n",
    "\n",
    "predictive = Predictive(model, guide=guide, num_samples=800,\n",
    "                        return_sites=(\"linear.weight\", \"obs\", \"_RETURN\"))\n",
    "samples = predictive(x_data)\n",
    "pred_summary = summary(samples)\n",
    "\n",
    "mu = pred_summary[\"_RETURN\"]\n",
    "y = pred_summary[\"obs\"]\n",
    "predictions = pd.DataFrame({\n",
    "    \"cont_africa\": x_data[:, 0],\n",
    "    \"rugged\": x_data[:, 1],\n",
    "    \"mu_mean\": mu[\"mean\"],\n",
    "    \"mu_perc_5\": mu[\"5%\"],\n",
    "    \"mu_perc_95\": mu[\"95%\"],\n",
    "    \"y_mean\": y[\"mean\"],\n",
    "    \"y_perc_5\": y[\"5%\"],\n",
    "    \"y_perc_95\": y[\"95%\"],\n",
    "    \"true_gdp\": y_data,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6), sharey=True)\n",
    "african_nations = predictions[predictions[\"cont_africa\"] == 1]\n",
    "non_african_nations = predictions[predictions[\"cont_africa\"] == 0]\n",
    "african_nations = african_nations.sort_values(by=[\"rugged\"])\n",
    "non_african_nations = non_african_nations.sort_values(by=[\"rugged\"])\n",
    "fig.suptitle(\"Regression line 90% CI\", fontsize=16)\n",
    "ax[0].plot(non_african_nations[\"rugged\"],\n",
    "           non_african_nations[\"mu_mean\"])\n",
    "ax[0].fill_between(non_african_nations[\"rugged\"],\n",
    "                   non_african_nations[\"mu_perc_5\"],\n",
    "                   non_african_nations[\"mu_perc_95\"],\n",
    "                   alpha=0.5)\n",
    "ax[0].plot(non_african_nations[\"rugged\"],\n",
    "           non_african_nations[\"true_gdp\"],\n",
    "           \"o\")\n",
    "ax[0].set(xlabel=\"Terrain Ruggedness Index\",\n",
    "          ylabel=\"log GDP (2000)\",\n",
    "          title=\"Non African Nations\")\n",
    "idx = np.argsort(african_nations[\"rugged\"])\n",
    "ax[1].plot(african_nations[\"rugged\"],\n",
    "           african_nations[\"mu_mean\"])\n",
    "ax[1].fill_between(african_nations[\"rugged\"],\n",
    "                   african_nations[\"mu_perc_5\"],\n",
    "                   african_nations[\"mu_perc_95\"],\n",
    "                   alpha=0.5)\n",
    "ax[1].plot(african_nations[\"rugged\"],\n",
    "           african_nations[\"true_gdp\"],\n",
    "           \"o\")\n",
    "ax[1].set(xlabel=\"Terrain Ruggedness Index\",\n",
    "          ylabel=\"log GDP (2000)\",\n",
    "          title=\"African Nations\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = samples[\"linear.weight\"]\n",
    "weight = weight.reshape(weight.shape[0], 3)\n",
    "gamma_within_africa = weight[:, 1] + weight[:, 2]\n",
    "gamma_outside_africa = weight[:, 1]\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "sns.distplot(gamma_within_africa, kde_kws={\"label\": \"African nations\"},)\n",
    "sns.distplot(gamma_outside_africa, kde_kws={\"label\": \"Non-African nations\"})\n",
    "fig.suptitle(\"Density of Slope : log(GDP) vs. Terrain Ruggedness\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from torch.distributions import constraints\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(is_cont_africa, ruggedness, log_gdp):\n",
    "    a = pyro.sample(\"a\", dist.Normal(0., 10.))\n",
    "    b_a = pyro.sample(\"bA\", dist.Normal(0., 1.))\n",
    "    b_r = pyro.sample(\"bR\", dist.Normal(0., 1.))\n",
    "    b_ar = pyro.sample(\"bAR\", dist.Normal(0., 1.))\n",
    "    sigma = pyro.sample(\"sigma\", dist.Uniform(0., 10.))\n",
    "    mean = a + b_a*is_cont_africa + b_r*ruggedness + b_ar*is_cont_africa*ruggedness\n",
    "    with pyro.plate(\"data\", len(ruggedness)):\n",
    "        pyro.sample(\"obs\", dist.Normal(mean, sigma), obs = log_gdp)\n",
    "\n",
    "def guide(is_cont_africa, ruggedness, log_gdp):\n",
    "    a_loc = pyro.param('a_loc', torch.tensor(0.))\n",
    "    a_scale = pyro.param('a_scale', torch.tensor(1.), constraint = constraints.positive)\n",
    "    sigma_loc = pyro.param('sigma_loc', torch.tensor(1.), constraint = constraints.positive)\n",
    "    weights_loc = pyro.param('weights_loc', torch.randn(3))\n",
    "    weights_scale = pyro.param('weights_scale', torch.ones(3), constraint = constraints.positive)\n",
    "\n",
    "    a = pyro.sample(\"a\", dist.Normal(a_loc, a_scale))\n",
    "    b_a = pyro.sample(\"bA\", dist.Normal(weights_loc[0], weights_scale[0]))\n",
    "    b_r = pyro.sample(\"bR\", dist.Normal(weights_loc[1], weights_scale[1]))\n",
    "    b_ar = pyro.sample(\"bAR\", dist.Normal(weights_loc[2], weights_scale[2]))\n",
    "    sigma = pyro.sample(\"sigma\", dist.Normal(sigma_loc, torch.tensor(0.05)))\n",
    "    mean = a + b_a * is_cont_africa + b_r * ruggedness + b_ar * is_cont_africa * ruggedness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(samples):\n",
    "    site_stats = {}\n",
    "    for site_name, values in samples.items():\n",
    "        marginal_site = pd.DataFrame(values)\n",
    "        describe = marginal_site.describe(percentiles = [.05, 0.25, 0.5, 0.75, 0.95]).transpose()\n",
    "        site_stats[site_name] = describe[[\"mean\", \"std\", \"5%\", \"25%\", \"50%\", \"75%\", \"95%\"]]\n",
    "    return site_stats\n",
    "\n",
    "\n",
    "DATA_URL = \"https://d2hg8soec8ck9v.cloudfront.net/datasets/rugged_data.csv\"\n",
    "rugged_data = pd.read_csv(DATA_URL, encoding=\"ISO-8859-1\")\n",
    "\n",
    "df = rugged_data[[\"cont_africa\", \"rugged\", \"rgdppc_2000\"]]\n",
    "df = df[np.isfinite(df.rgdppc_2000)]\n",
    "df[\"rgdppc_2000\"] = np.log(df[\"rgdppc_2000\"])\n",
    "train = torch.tensor(df.values, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer import SVI, Trace_ELBO\n",
    "\n",
    "svi = SVI(model, guide, optim.Adam({\"lr\": .05}), loss = Trace_ELBO())\n",
    "\n",
    "is_cont_africa, ruggedness, log_gdp = train[:, 0], train[:, 1], train[:, 2]\n",
    "pyro.clear_param_store()\n",
    "num_iters = 5000 if not smoke_test else 2\n",
    "for i in range(num_iters):\n",
    "    elbo = svi.step(is_cont_africa, ruggedness, log_gdp)\n",
    "    if i % 500 == 0:\n",
    "        logging.info(\"ELBO loss: {}\".format(elbo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer import Predictive\n",
    "\n",
    "num_samples = 1000\n",
    "predictive = Predictive(model, guide = guide, num_samples = num_samples)\n",
    "svi_sample = {k: v.reshape(num_samples).detach().cpu().numpy() for k, v in predictive(log_gdp, is_cont_africa, ruggedness).items() if k != \"obs\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(svi_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for site, values in summary(svi_sample).items():\n",
    "    print(\"Site: {}\".format(site))\n",
    "    print(values, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer import MCMC, NUTS\n",
    "\n",
    "nuts_kernel = NUTS(model)\n",
    "mcmc = MCMC(nuts_kernel, num_samples = 1000, warmup_steps = 200)\n",
    "mcmc.run(is_cont_africa, ruggedness, log_gdp)\n",
    "\n",
    "hmc_samples = {k: v.detach().cpu().numpy() for k, v in mcmc.get_samples().items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for site, values in summary(hmc_samples).items():\n",
    "    print(\"Site: {}\".format(site))\n",
    "    print(values, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_regression\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import MCMC, NUTS, Predictive\n",
    "from pyro.infer.mcmc.util import summary\n",
    "from pyro.distributions import constraints\n",
    "import pyro\n",
    "import torch\n",
    "\n",
    "pyro.set_rng_seed(101)\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(n_features=1, bias=150., noise=5., random_state=108)\n",
    "X_ = torch.tensor(X, dtype=torch.float)\n",
    "y_ = torch.tensor((y**3)/100000. + 10., dtype=torch.float)\n",
    "y_.round_().clamp_(min=0)\n",
    "plt.scatter(X_, y_)\n",
    "plt.ylabel('y')\n",
    "plt.xlabel('x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(features, counts):\n",
    "    N, P = features.shape\n",
    "    scale = pyro.sample(\"scale\", dist.LogNormal(0, 1))\n",
    "    coef = pyro.sample(\"coef\", dist.Normal(0, scale).expand([P]).to_event(1))\n",
    "    rate = pyro.deterministic(\"rate\", torch.nn.functional.softplus(coef @ features.T))\n",
    "    concentration = pyro.sample(\"concentration\", dist.LogNormal(0, 1))\n",
    "    with pyro.plate(\"bins\", N):\n",
    "        return pyro.sample(\"counts\", dist.GammaPoisson(concentration, rate), obs = counts)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuts_kernel = NUTS(model)\n",
    "mcmc = MCMC(nuts_kernel, num_samples=500)\n",
    "\n",
    "mcmc.run(X_, y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = mcmc.get_samples()\n",
    "for k, v in samples.items():\n",
    "    print(f\"{k}: {tuple(v.shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictive = Predictive(model, samples)(X_, None)\n",
    "for k, v in predictive.items():\n",
    "    print(f\"{k}: {tuple(v.shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_counts_df(predictive):\n",
    "    counts = predictive['counts'].numpy()\n",
    "    counts_mean = counts.mean(axis=0)\n",
    "    counts_std = counts.std(axis=0)\n",
    "\n",
    "    counts_df = pd.DataFrame({\n",
    "    \"feat\": X_.squeeze(),\n",
    "    \"mean\": counts_mean,\n",
    "    \"high\": counts_mean + counts_std,\n",
    "    \"low\": counts_mean - counts_std,\n",
    "    })\n",
    "\n",
    "    return counts_df.sort_values(by=['feat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_df = prepare_counts_df(predictive)\n",
    "plt.scatter(X_, y_, c='r')\n",
    "plt.ylabel('y')\n",
    "plt.xlabel('x')\n",
    "plt.plot(counts_df['feat'], counts_df['mean'])\n",
    "plt.fill_between(counts_df['feat'], counts_df['high'], counts_df['low'], alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "from pyro.infer.autoguide import AutoNormal\n",
    "\n",
    "def guide(features, counts):\n",
    "    N, P = features.shape\n",
    "\n",
    "    scale_param = pyro.param(\"scale_param\", torch.tensor(0.1), constraint = constraints.positive)\n",
    "    loc_param = pyro.param(\"loc_param\", torch.tensor(0.0))\n",
    "    scale = pyro.sample(\"scale\", dist.Delta(scale_param))\n",
    "    coef = pyro.sample(\"coef\", dist.Normal(loc_param, scale).expand([P]).to_event(1))\n",
    "\n",
    "    concentration_param = pyro.param(\"concentration_param\", torch.tensor(0.1), constraint = constraints.positive)\n",
    "    concentration = pyro.sample(\"concentration\", dist.Delta(concentration_param))\n",
    "\n",
    "pyro.clear_param_store()\n",
    "\n",
    "adam_params = {\"lr\": 0.005, \"betas\": (0.90, 0.999)}\n",
    "optimizer = Adam(adam_params)\n",
    "\n",
    "svi = SVI(model, guide, optimizer, loss = Trace_ELBO())\n",
    "\n",
    "n_steps = 5001\n",
    "\n",
    "for step in range(n_steps):\n",
    "    loss = svi.step(X_, y_)\n",
    "    if step % 1000 == 0:\n",
    "        print('Loss: ', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(pyro.get_param_store().items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictive_svi = Predictive(model, guide=guide, num_samples=500)(X_, None)\n",
    "for k, v in predictive_svi.items():\n",
    "    print(f\"{k}: {tuple(v.shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_df = prepare_counts_df(predictive_svi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PCW_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "635118ddd23069179fb6d4754b27b83b38a3ab9ec0315a2e37cc1274fa283920"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
